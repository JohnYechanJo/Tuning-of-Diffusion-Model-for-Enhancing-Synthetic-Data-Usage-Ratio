{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WCqhfNojkT1l"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNLFrCHMI25p55DH57tBMXG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnYechanJo/Tuning-of-Diffusion-Model-for-Enhancing-Synthetic-Data-Usage-Ratio/blob/main/2_nd_demo(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Train on both domain(Kermany and OCT5K) and test the gap"
      ],
      "metadata": {
        "id": "Zi_q9XdfXV_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.1 Training Simple CNN classifier\n"
      ],
      "metadata": {
        "id": "T-UTYs00JmgU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcs1Av2ESUor",
        "outputId": "5b62a03a-4789-4c34-b72d-f2344f4dcdaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.31.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting pytorch-fid\n",
            "  Downloading pytorch_fid-0.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.7.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "INFO: pip is looking at multiple versions of xformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.31-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.7.14)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl (43.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_fid-0.3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, xformers, pytorch-fid\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-fid-0.3.0 xformers-0.0.29.post3\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting torch-fidelity\n",
            "  Downloading torch_fidelity-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (11.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (1.16.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (4.67.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.8.0-py3-none-any.whl (981 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.9/981.9 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, torch-fidelity\n",
            "Successfully installed lightning-utilities-0.15.2 torch-fidelity-0.3.0 torchmetrics-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install kagglehub torch torchvision transformers diffusers accelerate datasets xformers pytorch-fid pandas\n",
        "!pip install torchmetrics --upgrade torch-fidelity\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import kagglehub\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from torch.utils.data import Sampler\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "from transformers import ViTModel, CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler, StableDiffusionPipeline\n",
        "from accelerate import Accelerator\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_fid import fid_score\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "# Set random seed\n",
        "seed = 123\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# GPU setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "# Memory cleanup function\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.2 Dataset remerge(SKIP)\n",
        "Place the code file in the extracted AMD/DME/Normal directory, modify the `prefix` accordingly, and then run it. url: `https://drive.google.com/uc?id=1ZK_qMNuy53Nfxjo2vShHMFz2LyGwYAuW`\n",
        "then upload the file on drive"
      ],
      "metadata": {
        "id": "WCqhfNojkT1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os\n",
        "# set this\n",
        "prefix = 'Normal'\n",
        "def merge_all_image_dirs_under(root_dir, dst_dir, prefix=None):\n",
        "    os.makedirs(dst_dir, exist_ok=True)\n",
        "    global_index = 0\n",
        "\n",
        "    src_dirs = [\n",
        "        os.path.join(root_dir, d)\n",
        "        for d in os.listdir(root_dir)\n",
        "        if os.path.isdir(os.path.join(root_dir, d))\n",
        "    ]\n",
        "\n",
        "    for index, src_dir in enumerate(src_dirs):\n",
        "        for fname in os.listdir(src_dir):\n",
        "            if fname.lower().endswith(('.tiff')):\n",
        "                src_path = os.path.join(src_dir, fname)\n",
        "\n",
        "                if prefix:\n",
        "                    new_fname = f\"{prefix}_{global_index}.TIFF\"\n",
        "                else:\n",
        "                    new_fname = f\"{global_index}.TIFF\"\n",
        "\n",
        "                dst_path = os.path.join(dst_dir, new_fname)\n",
        "                shutil.copy(src_path, dst_path)\n",
        "                global_index += 1\n",
        "                #print(f\"{global_index} - copied from {src_dir}\")\n",
        "\n",
        "    print(f\"Successfully merged: {global_index} images in total!\")\n",
        "\n",
        "\n",
        "\n",
        "root_dir = os.getcwd() + f\"/{prefix}\"\n",
        "dst_dir = os.getcwd() + f\"/merged_{prefix}\"\n",
        "\n",
        "merge_all_image_dirs_under(root_dir, dst_dir, prefix)\n"
      ],
      "metadata": {
        "id": "8jrAtxqNkTUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.3 Dataset load and Path set"
      ],
      "metadata": {
        "id": "9etCEjPvjT1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')# mount drive\n",
        "\n",
        "# download dataset\n",
        "path = kagglehub.dataset_download(\"paultimothymooney/kermany2018\")\n",
        "loadpath = os.path.join(path, \"OCT2017 /train\")\n",
        "train_path_dme = os.path.join(loadpath, \"DME\")\n",
        "train_path_normal = os.path.join(loadpath, \"NORMAL\")\n",
        "lp = os.path.join(path, \"OCT2017 /val\")\n",
        "val_path_dme = os.path.join(lp, \"DME\")\n",
        "val_path_normal = os.path.join(lp, \"NORMAL\")\n",
        "lpp = os.path.join(path, \"OCT2017 /test\")\n",
        "test_path_dme = os.path.join(lpp, \"DME\")\n",
        "test_path_normal = os.path.join(lpp, \"NORMAL\")\n",
        "# get OCT5K from drive /content/drive/MyDrive\n",
        "target_dme = '/content/drive/MyDrive/OCT5K/DME'\n",
        "target_normal = '/content/drive/MyDrive/OCT5K/Normal'\n",
        "# 1600:1600->train 160(80:80)*20 / 8:8->val / 242:242 origin test\n",
        "# target test 1000:1000\n",
        "# define\n",
        "'''\n",
        "bsz = 160\n",
        "half_bsz = bsz // 2\n",
        "batch_num = 20\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "TH32Be2vdOna",
        "outputId": "a3a8368e-ddc6-41d2-ea34-606b2e50fe1b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/paultimothymooney/kermany2018?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10.8G/10.8G [01:57<00:00, 99.3MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nbsz = 160\\nhalf_bsz = bsz // 2\\nbatch_num = 20\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.4 Dataloader Definition"
      ],
      "metadata": {
        "id": "5NvFpdqQW6GB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def img_transform():\n",
        "    return transforms.Compose([\n",
        "        transforms.Lambda(lambda img: img.crop((0, 100, 768, 400)) if img.size[0] >= 768 and img.size[1] >= 400 else img),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    #3，224，224\n",
        "class OCTBinaryDataset(Dataset):\n",
        "    def __init__(self, dme_dir, normal_dir, transform=None, max_per_class=None):\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "\n",
        "        dme_images = sorted([os.path.join(dme_dir, f) for f in os.listdir(dme_dir)\n",
        "                             if f.lower().endswith(('.jpg', '.jpeg', '.png','.tiff'))])\n",
        "        normal_images = sorted([os.path.join(normal_dir, f) for f in os.listdir(normal_dir)\n",
        "                                if f.lower().endswith(('.jpg', '.jpeg', '.png','.tiff'))])\n",
        "\n",
        "        if max_per_class:\n",
        "            dme_images = dme_images[:max_per_class]\n",
        "            normal_images = normal_images[:max_per_class]\n",
        "\n",
        "        self.dme_paths = dme_images\n",
        "        self.normal_paths = normal_images\n",
        "        assert len(self.dme_paths) > 0, f\"No images found in {dme_dir}\"\n",
        "        assert len(self.normal_paths) > 0, f\"No images found in {normal_dir}\"\n",
        "        self.samples += [(path, 1) for path in dme_images]      # DME: label 0\n",
        "        self.samples += [(path, 0) for path in normal_images]   # NORMAL: label 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "class BalancedBinaryBatchSampler(Sampler):\n",
        "    def __init__(self, dme_len, normal_len, batch_size, num_batches=None, seed = 123):\n",
        "        assert batch_size % 2 == 0, \"Batch size must be divisible by 2\"\n",
        "        self.dme_len = dme_len\n",
        "        self.normal_len = normal_len\n",
        "        self.half_batch = batch_size // 2\n",
        "        self.num_batches = num_batches or min(dme_len, normal_len) // self.half_batch\n",
        "        self.seed = seed\n",
        "        assert self.dme_len >= self.half_batch * self.num_batches, \"Not enough DME samples\"\n",
        "        assert self.normal_len >= self.half_batch * self.num_batches, \"Not enough NORMAL samples\"\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        rng = random.Random(self.seed)\n",
        "        dme_indices = list(range(self.dme_len))\n",
        "        normal_indices = list(range(self.normal_len))\n",
        "        rng.shuffle(dme_indices)\n",
        "        rng.shuffle(normal_indices)\n",
        "\n",
        "        for i in range(self.num_batches):\n",
        "            dme_batch = dme_indices[i*self.half_batch:(i+1)*self.half_batch]\n",
        "            normal_batch = normal_indices[i*self.half_batch:(i+1)*self.half_batch]\n",
        "            yield dme_batch + [j + self.dme_len for j in normal_batch]  # shift normal index\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_batches\n",
        "\n",
        "def build_train_dataloader(dme_dir, normal_dir, batch_size=160, num_batches=20, transform=None, max_per_class=None):\n",
        "    transform = transform or img_transform()\n",
        "    dataset = OCTBinaryDataset(dme_dir, normal_dir, transform=transform, max_per_class=max_per_class)\n",
        "    dme_len = len(dataset.dme_paths)\n",
        "    normal_len = len(dataset.normal_paths)\n",
        "    sampler = BalancedBinaryBatchSampler(dme_len, normal_len, batch_size=batch_size, num_batches = num_batches)\n",
        "    def balanced_collate_fn(batch):\n",
        "        random.shuffle(batch)\n",
        "        imgs, labels = zip(*batch)\n",
        "        return torch.stack(imgs), torch.tensor(labels)\n",
        "    loader = DataLoader(dataset, batch_sampler=sampler,\n",
        "                        collate_fn=balanced_collate_fn,\n",
        "                        num_workers=2, pin_memory=True)\n",
        "    return loader\n",
        "def build_valandtest_dataloader(dme_dir, normal_dir, batch_size, transform=None, max_per_class=None):\n",
        "    transform = transform or img_transform()\n",
        "    dataset = OCTBinaryDataset(dme_dir, normal_dir, transform=transform, max_per_class=max_per_class)\n",
        "    dme_len = len(dataset.dme_paths)\n",
        "    normal_len = len(dataset.normal_paths)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size,\n",
        "                        shuffle=False,\n",
        "                        num_workers=2, pin_memory=True)\n",
        "    return loader\n",
        "\n",
        "train_loader = build_train_dataloader(\n",
        "    dme_dir= train_path_dme ,\n",
        "    normal_dir= train_path_normal,\n",
        "    batch_size=160,\n",
        "    num_batches = 20,\n",
        "    transform=img_transform(),\n",
        "    max_per_class=1600\n",
        ")\n",
        "\n",
        "val_loader = build_valandtest_dataloader(\n",
        "    dme_dir= val_path_dme ,\n",
        "    normal_dir= val_path_normal,\n",
        "    batch_size=8,\n",
        "    transform=img_transform(),\n",
        "    max_per_class = 8\n",
        ")\n",
        "\n",
        "test_loader_origin = build_valandtest_dataloader(\n",
        "    dme_dir= test_path_dme ,\n",
        "    normal_dir= test_path_normal,\n",
        "    batch_size=242,\n",
        "    transform=img_transform(),\n",
        "    max_per_class = 242\n",
        ")\n",
        "\n",
        "# may cost extra time to load when test for the first time\n",
        "test_loader_target = build_valandtest_dataloader(\n",
        "    dme_dir= target_dme ,\n",
        "    normal_dir= target_normal,\n",
        "    batch_size=100,\n",
        "    transform=img_transform(),\n",
        "    max_per_class = 1000\n",
        ")\n"
      ],
      "metadata": {
        "id": "4JUssCqwe5mZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.5 Clssifier Definition"
      ],
      "metadata": {
        "id": "lD-Hvx8ejrqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),  # (3,224,224) -> (16,224,224)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                            # -> (16,112,112)\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1), # -> (32,112,112)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                             # -> (32,56,56)\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # -> (64,56,56)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)                              # -> (64,28,28)\n",
        "        )\n",
        "        self.flatten = nn.Flatten() # (64*28*28)\n",
        "        self.embedding = nn.Linear(64*28*28, 128)  # used for prototype\n",
        "        self.classifier = nn.Linear(128, 2)        # 2-class classification\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block(x)\n",
        "        x = self.flatten(x)\n",
        "        feat = self.embedding(x)     # prototype feature\n",
        "        logits = self.classifier(F.relu(feat))\n",
        "        return logits, feat          # return both\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, train_loader, val_loader):\n",
        "        super().__init__()\n",
        "        self.model = SimpleCNN().cuda()\n",
        "        self.best_acc = 0\n",
        "        self.model_path = \"/content/drive/MyDrive/Classifier_model/best_model.pth\"\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "\n",
        "\n",
        "    def load_model(self):\n",
        "        if os.path.exists(self.model_path):\n",
        "            print(f\"Found saved model at {self.model_path}, loading...\")\n",
        "            checkpoint = torch.load(self.model_path,map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.model.eval()\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def train_val(self):\n",
        "      if self.load_model():\n",
        "        print(\"Skipping training, using loaded model.\")\n",
        "      else:\n",
        "        print(\"No saved model found, starting training.\")\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
        "        loss = nn.CrossEntropyLoss()\n",
        "        epochs = 10\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "            # train\n",
        "            self.model.train()\n",
        "            for i, data in enumerate(self.train_loader):\n",
        "                total = len(self.train_loader)\n",
        "                batch_x, batch_y = (item.cuda() for item in data)\n",
        "                self.optimizer.zero_grad()\n",
        "                logit_original,_ = self.model(batch_x)\n",
        "                l = loss(logit_original, batch_y)\n",
        "                l.backward()\n",
        "                self.optimizer.step()\n",
        "                corrects = (torch.max(logit_original, 1)[1].view(batch_y.size()).data == batch_y.data).sum()\n",
        "                accuracy = 100 * corrects / len(batch_y)\n",
        "                print(f'Batch[{i + 1}/{total}] - loss: {l.item():.6f}  accuracy: {accuracy:.4f}%({corrects}/{batch_y.size(0)})')\n",
        "            # val\n",
        "            self.model.eval()\n",
        "            all_preds, all_labels = [], []\n",
        "            with torch.no_grad():\n",
        "                for x, y in self.val_loader:\n",
        "                    x = x.cuda()\n",
        "                    logits,_ = self.model(x)\n",
        "                    preds = torch.argmax(logits, dim=1)\n",
        "                    all_preds.extend(preds.cpu().tolist())\n",
        "                    all_labels.extend(y.tolist())\n",
        "\n",
        "                acc = accuracy_score(all_labels, all_preds)\n",
        "                print(f\"Validation Accuracy: {acc:.4f}\")\n",
        "                if acc > self.best_acc:\n",
        "                   self.best_acc = acc\n",
        "                   os.makedirs(os.path.dirname(self.model_path), exist_ok=True)\n",
        "                   torch.save({\n",
        "                        'model_state_dict': self.model.state_dict()\n",
        "                    }, self.model_path)\n",
        "                   print(f\"Model saved at {self.model_path}\")\n",
        "                print(\"Best val set acc:\", self.best_acc)\n",
        "\n",
        "\n",
        "    # test\n",
        "    def test(self, loader, test_name=\"\"):\n",
        "       self.model.eval()\n",
        "       all_preds, all_labels = [], []\n",
        "       start_time = time.time()\n",
        "       print(f\"> {test_name} Start test!\")\n",
        "\n",
        "       with torch.no_grad():\n",
        "         for i,data in enumerate(loader):\n",
        "            t0 = time.time()\n",
        "            x,y = (item.cuda(non_blocking=True) for item in data)\n",
        "            t1 = time.time()\n",
        "            logits,_= self.model(x)\n",
        "            t2 = time.time()\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().tolist())\n",
        "            all_labels.extend(y.tolist())\n",
        "            t3 = time.time()\n",
        "            print(f\"[Batch {i}] | load: {t1 - t0:.3f}s | forward: {t2 - t1:.3f}s | predict: {t3 - t2:.3f}s \")\n",
        "       report = classification_report(\n",
        "           all_labels, all_preds,\n",
        "           labels=[0, 1],\n",
        "           target_names=['NR', 'FR'],\n",
        "           digits=3,\n",
        "           output_dict=True\n",
        "       )\n",
        "       print(f\"{test_name} result: {report['accuracy']:.4f}\")\n",
        "       total_time = time.time() - start_time\n",
        "       print(f\"> {test_name} Finished Spend: {total_time:.2f}s in total\")\n",
        "       return report\n",
        "\n",
        "    def get_prototypes(self, loader): # use this method after trained\n",
        "       self.model.eval()\n",
        "       features = {0: [], 1: []}\n",
        "       with torch.no_grad():\n",
        "         for x, y in loader:\n",
        "            x, y = x.cuda(), y.cuda()\n",
        "            _, feats = model(x)  # [, 128]\n",
        "            for i in range(len(y)):\n",
        "                features[y[i].item()].append(feats[i].cpu())\n",
        "\n",
        "       prototypes = {}\n",
        "       for cls in features:\n",
        "         if len(features[cls]) > 0:\n",
        "            prototypes[cls] = torch.stack(features[cls], dim=0).mean(dim=0)  # [128]\n",
        "       return prototypes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def test_TTA(self, dataloader, prototypes, threshold=0.9, momentum=0.8, test_name=\"TTA\"):\n",
        "\n",
        "       self.model.eval()\n",
        "       all_preds, all_labels = [], []\n",
        "\n",
        "       start_time = time.time()\n",
        "       with torch.no_grad():\n",
        "        for i, data in enumerate(dataloader):\n",
        "            x, y = (item.cuda(non_blocking=True) for item in data)\n",
        "\n",
        "            logits, feats = model(x)  # logits: [B, C], feats: [B, D]\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            max_probs, pseudo_labels = torch.max(probs, dim=1)\n",
        "\n",
        "            feats = feats.cpu()\n",
        "            pseudo_labels = pseudo_labels.cpu()\n",
        "            max_probs = max_probs.cpu()\n",
        "            y = y.cpu()\n",
        "\n",
        "            batch_preds = []\n",
        "            for j in range(len(x)):\n",
        "                feat = feats[j]\n",
        "                pseudo_label = pseudo_labels[j].item()\n",
        "                conf = max_probs[j].item()\n",
        "\n",
        "                # Prototype-based prediction\n",
        "                distances = [torch.norm(feat - prototypes[c]) for c in prototypes]\n",
        "                pred = min(zip(prototypes.keys(), distances), key=lambda x: x[1])[0]\n",
        "\n",
        "                batch_preds.append(pred)\n",
        "                all_labels.append(y[j].item())\n",
        "\n",
        "                # Prototype update\n",
        "                if conf >= threshold:\n",
        "                    if pseudo_label in prototypes:\n",
        "                        prototypes[pseudo_label] = momentum * prototypes[pseudo_label] + (1 - momentum) * feat\n",
        "                    else:\n",
        "                        prototypes[pseudo_label] = feat\n",
        "\n",
        "            all_preds.extend(batch_preds)\n",
        "\n",
        "\n",
        "            batch_report = classification_report(\n",
        "                y.tolist(), batch_preds,\n",
        "                labels=[0, 1],\n",
        "                target_names=['NR', 'FR'],\n",
        "                digits=3\n",
        "            )\n",
        "            print(f\"\\n[Batch {i}] Classification Report:\\n{batch_report}\")\n",
        "\n",
        "\n",
        "       report = classification_report(\n",
        "        all_labels, all_preds,\n",
        "        labels=[0, 1],\n",
        "        target_names=['NR', 'FR'],\n",
        "        digits=3,\n",
        "        output_dict=True\n",
        "       )\n",
        "       print(f\"\\n{test_name} result: {report['accuracy']:.4f}\")\n",
        "       print(f\"> {test_name} Finished Spend: {time.time() - start_time:.2f}s in total\")\n",
        "\n",
        "       return report\n"
      ],
      "metadata": {
        "id": "5i_6uTgFdcDA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. CNN-Based VAE"
      ],
      "metadata": {
        "id": "jabI_wxzis8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.1 Dataloader"
      ],
      "metadata": {
        "id": "t_alDV0j34_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Subset\n",
        "import shutil\n",
        "import torch.nn.functional as F\n",
        "from collections import defaultdict\n",
        "def img_transform():\n",
        "    return transforms.Compose([\n",
        "        transforms.Lambda(lambda img: img.crop((0, 100, 768, 400)) if img.size[0] >= 768 and img.size[1] >= 400 else img),\n",
        "        transforms.Resize((256,256)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "def get_limited_indices(dataset, target_labels, max_per_class):\n",
        "    label_counts = defaultdict(int)\n",
        "    selected_indices = []\n",
        "\n",
        "    for idx, label in enumerate(dataset.targets):\n",
        "        if label in target_labels and label_counts[label] < max_per_class:\n",
        "            selected_indices.append(idx)\n",
        "            label_counts[label] += 1\n",
        "\n",
        "        # early stop\n",
        "        if all(label_counts[l] >= max_per_class for l in target_labels):\n",
        "            break\n",
        "\n",
        "    return selected_indices\n",
        "\n",
        "if os.path.exists('/content/OCT5K'):\n",
        "    shutil.rmtree('/content/OCT5K')\n",
        "shutil.copytree('/content/drive/MyDrive/OCT5K', '/content/OCT5K')\n",
        "lp_target = '/content/OCT5K'\n",
        "transform = img_transform()\n",
        "origin_dataset = ImageFolder(root=loadpath, transform=transform)\n",
        "target_dataset = ImageFolder(root=lp_target, transform=transform)\n",
        "print(origin_dataset.class_to_idx)\n",
        "print(target_dataset.class_to_idx)\n",
        "# deal with iamgefolder issue: it treat each folder as an auto label for one class\n",
        "# but we do binary, since dme/normal is the maximum intersection between two datasets\n",
        "max_images_per_class = 1000\n",
        "origin_indices = get_limited_indices(origin_dataset, target_labels=[1, 3], max_per_class=max_images_per_class)\n",
        "target_indices = get_limited_indices(target_dataset, target_labels=[1, 2], max_per_class=max_images_per_class)\n",
        "origin_subset = Subset(origin_dataset, origin_indices)\n",
        "target_subset = Subset(target_dataset, target_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sx3DvLKoitcG",
        "outputId": "26897c08-7425-4457-9ee6-8966b87c81b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'CNV': 0, 'DME': 1, 'DRUSEN': 2, 'NORMAL': 3}\n",
            "{'AMD': 0, 'DME': 1, 'Normal': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "origin_loader = DataLoader(origin_subset, batch_size=16, shuffle=True, num_workers=2)\n",
        "target_loader = DataLoader(target_subset, batch_size=16, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "C6E05wcAqFzX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For prototype strategy: a full set"
      ],
      "metadata": {
        "id": "r7p0tv9wTlUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_indices = [i for i, (_, y) in enumerate(origin_dataset) if y in [1,3]]\n",
        "class RelabeledSubset(Dataset):\n",
        "    def __init__(self, dataset, indices, label_map):\n",
        "        self.subset = Subset(dataset, indices)\n",
        "        self.label_map = label_map\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        return x, self.label_map[y]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "label_map = {1: 1, 3: 0}  # DME → 1, NORMAL → 0\n",
        "full_origin_dataset = RelabeledSubset(origin_dataset, full_indices, label_map)"
      ],
      "metadata": {
        "id": "eAyZdpZ-RxW1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_origin_loader = DataLoader(full_origin_dataset, batch_size=32, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "FEB9FSDeThv2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80eee182-035a-4eed-8a1e-8bb6c969b0ce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.2 VAE Definition"
      ],
      "metadata": {
        "id": "8vjfibh438yT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Residualblock"
      ],
      "metadata": {
        "id": "kt2cdiG4LznD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout2d(0.1),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "        self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=1) \\\n",
        "            if in_channels != out_channels else nn.Identity()\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activation(self.block(x) + self.skip(x))"
      ],
      "metadata": {
        "id": "alSVKTDrLyoH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "vae-1(skip)"
      ],
      "metadata": {
        "id": "Eexf47ZfL6-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            ResidualBlock(3, 32),     # (B,3,256,256) -> (B,32,256,256)\n",
        "            nn.MaxPool2d(2),          # -> (B,32,128,128)\n",
        "\n",
        "            ResidualBlock(32, 64),    # -> (B,64,128,128)\n",
        "            nn.MaxPool2d(2),          # -> (B,64,64,64)\n",
        "\n",
        "            ResidualBlock(64, 128),   # -> (B,128,64,64)\n",
        "            nn.MaxPool2d(2),          # -> (B,128,32,32)\n",
        "\n",
        "            ResidualBlock(128, 256),  # -> (B,256,32,32)\n",
        "            nn.MaxPool2d(2),          # -> (B,256,16,16)\n",
        "\n",
        "            nn.Conv2d(256, latent_dim, kernel_size=1),  # -> (B,latent_dim,16,16)\n",
        "            nn.AdaptiveAvgPool2d((1, 1))                # -> (B,latent_dim,1,1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return x.view(x.size(0), -1)  # -> (B, latent_dim)\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "           nn.Linear(latent_dim, 256 * 8 * 8),\n",
        "           nn.LeakyReLU(0.1),\n",
        "           nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # -> (128,16,16)\n",
        "            ResidualBlock(128, 128),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # -> (64,32,32)\n",
        "            ResidualBlock(64, 64),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),    # -> (32,64,64)\n",
        "            ResidualBlock(32, 32),\n",
        "\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),    # -> (16,128,128)\n",
        "            ResidualBlock(16, 16),\n",
        "\n",
        "            nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=1),     # -> (3,256,256)\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)                        # -> (B,256*8*8)\n",
        "        x = x.view(x.size(0), 256, 8, 8)      # -> (B,256,8,8)\n",
        "        return self.decoder(x)\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(latent_dim=latent_dim)\n",
        "        self.fc_mu = nn.Linear(latent_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(latent_dim, latent_dim)\n",
        "        self.decoder_input = nn.Sequential(\n",
        "            nn.Linear(latent_dim, latent_dim),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "        self.decoder = Decoder(latent_dim=latent_dim)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        mu = self.fc_mu(z)\n",
        "        logvar = self.fc_logvar(z)\n",
        "        z_sample = self.reparameterize(mu, logvar)\n",
        "        x_recon = self.decoder(self.decoder_input(z_sample))\n",
        "        return x_recon, mu, logvar\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q6rMqMwB4B26"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "vae-2: skip-connection"
      ],
      "metadata": {
        "id": "mca_rNTrL9RL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.res1 = ResidualBlock(3, 32)       # -> (B,32,256,256)\n",
        "        self.pool1 = nn.MaxPool2d(2)           # -> (B,32,128,128)\n",
        "\n",
        "        self.res2 = ResidualBlock(32, 64)      # -> (B,64,128,128)\n",
        "        self.pool2 = nn.MaxPool2d(2)           # -> (B,64,64,64)\n",
        "\n",
        "        self.res3 = ResidualBlock(64, 128)     # -> (B,128,64,64)\n",
        "        self.pool3 = nn.MaxPool2d(2)           # -> (B,128,32,32)\n",
        "\n",
        "        self.res4 = ResidualBlock(128, 256)    # -> (B,256,32,32)\n",
        "        self.pool4 = nn.MaxPool2d(2)           # -> (B,256,16,16)\n",
        "\n",
        "        '''self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(256, latent_dim, kernel_size=1),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )'''\n",
        "        self.bottleneck = nn.Conv2d(256, latent_dim, kernel_size=1)  # → (B, latent_dim, 16, 16)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skips = []\n",
        "        x = self.res1(x); skips.append(x); x = self.pool1(x)\n",
        "        x = self.res2(x); skips.append(x); x = self.pool2(x)\n",
        "        x = self.res3(x); skips.append(x); x = self.pool3(x)\n",
        "        x = self.res4(x); skips.append(x); x = self.pool4(x)\n",
        "        z = self.bottleneck(x)\n",
        "        return z, skips\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim=64):\n",
        "        super().__init__()\n",
        "        '''self.fc = nn.Sequential(\n",
        "        nn.Linear(latent_dim, 256 * 16 * 16),\n",
        "        nn.LeakyReLU(0.1),\n",
        "        nn.Dropout(0.2)\n",
        "        )'''\n",
        "        self.fc = nn.Identity()\n",
        "\n",
        "        self.up4 = nn.ConvTranspose2d(256, 128, 4, 2, 1)  # -> (128,16,16)\n",
        "        self.res4 = ResidualBlock(128 , 128)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(128, 64, 4, 2, 1)   # -> (64,32,32)\n",
        "        self.res3 = ResidualBlock(64 + 128, 64)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(64, 32, 4, 2, 1)    # -> (32,64,64)\n",
        "        self.res2 = ResidualBlock(32, 32)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(32, 16, 4, 2, 1)    # -> (16,128,128)\n",
        "        self.res1 = ResidualBlock(16 , 16)\n",
        "\n",
        "        self.final_up = nn.Conv2d(16, 3, kernel_size=3, padding=1)  # 不再上采样\n",
        "\n",
        "\n",
        "    def forward(self, z, skips):\n",
        "        #x = self.fc(z).view(z.size(0), 256, 16, 16)\n",
        "\n",
        "        x = self.up4(z)\n",
        "        x = self.res4(x)\n",
        "\n",
        "        x = self.up3(x)\n",
        "        x = self.res3(torch.cat([x, skips[2]], dim=1))\n",
        "\n",
        "        x = self.up2(x)\n",
        "        x = self.res2(x)\n",
        "\n",
        "        x = self.up1(x)\n",
        "        x = self.res1(x)\n",
        "\n",
        "        return torch.sigmoid(self.final_up(x))\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(latent_dim)\n",
        "        '''self.fc_mu = nn.Linear(latent_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(latent_dim, latent_dim)'''\n",
        "        self.fc_mu = nn.Conv2d(latent_dim, latent_dim, kernel_size=1)\n",
        "        self.fc_logvar = nn.Conv2d(latent_dim, latent_dim, kernel_size=1)\n",
        "\n",
        "        '''self.decoder_input = nn.Sequential(\n",
        "            nn.Linear(latent_dim, latent_dim),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )'''\n",
        "        self.decoder_input = nn.Identity()\n",
        "        self.decoder = Decoder(latent_dim)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, skips = self.encoder(x)\n",
        "        mu = self.fc_mu(z)\n",
        "        logvar = self.fc_logvar(z)\n",
        "        z_sample = self.reparameterize(mu, logvar)\n",
        "        x_recon = self.decoder(self.decoder_input(z_sample), skips)\n",
        "        return x_recon, mu, logvar\n"
      ],
      "metadata": {
        "id": "t3nv4fktLtcM"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.3 VAE Trainer Definition"
      ],
      "metadata": {
        "id": "_W5zSEcv_kSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "import torchvision.utils as vutils\n",
        "class VGGPerceptualLoss(nn.Module):\n",
        "    def __init__(self, layers=['relu3_3'], resize=True):\n",
        "        super().__init__()\n",
        "        vgg = models.vgg16(pretrained=True).features.eval()\n",
        "        self.resize = resize\n",
        "\n",
        "        self.selected_layers = {\n",
        "            'relu1_2': 3,\n",
        "            'relu2_2': 8,\n",
        "            'relu3_3': 15,\n",
        "            'relu4_3': 22,\n",
        "        }\n",
        "        self.layer_index = self.selected_layers[layers[0]]\n",
        "\n",
        "        self.vgg_part = nn.Sequential(*list(vgg.children())[:self.layer_index + 1])\n",
        "        for p in self.vgg_part.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).cuda()\n",
        "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).cuda()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "\n",
        "        if x.shape[1] == 1:\n",
        "            x = x.repeat(1, 3, 1, 1)\n",
        "            y = y.repeat(1, 3, 1, 1)\n",
        "\n",
        "        x = (x - self.mean) / self.std\n",
        "        y = (y - self.mean) / self.std\n",
        "\n",
        "        feat_x = self.vgg_part(x)\n",
        "        feat_y = self.vgg_part(y)\n",
        "        loss = nn.functional.l1_loss(feat_x, feat_y)\n",
        "        return loss\n",
        "\n",
        "\n",
        "class VAETrainer:\n",
        "    def __init__(self, model, dataloader, save_path_1, save_path_2,\n",
        "                 lr=2e-4, device='cuda', beta=1.0,\n",
        "                 recon_type='mse'):\n",
        "        self.model = model.to(device)\n",
        "        self.dataloader = dataloader\n",
        "        self.save_path_1 = save_path_1\n",
        "        self.save_path_2 = save_path_2\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        self.device = device\n",
        "        self.beta = beta\n",
        "        self.recon_type = recon_type\n",
        "        self.percep_loss = VGGPerceptualLoss().to(self.device)\n",
        "        self.fid_metric = FrechetInceptionDistance(feature=2048).to(self.device)\n",
        "\n",
        "    def loss_f(self, recon_x, x, mu, logvar):\n",
        "        if self.recon_type == 'mse':\n",
        "            recon_loss = F.mse_loss(recon_x, x, reduction='mean')\n",
        "        elif self.recon_type == 'l1':\n",
        "            recon_loss = F.l1_loss(recon_x, x, reduction='mean')\n",
        "        else:\n",
        "            raise ValueError(\"recon_type must be 'mse' or 'l1'\")\n",
        "\n",
        "        kl_div = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        kl_div = torch.clamp(kl_div, min=0.01)\n",
        "        #L1_loss = F.l1_loss(recon_x, x, reduction='mean')\n",
        "        percep = self.percep_loss(recon_x, x)\n",
        "        total_loss = recon_loss  +  0.08 * percep + self.beta * kl_div\n",
        "        return total_loss, recon_loss.item(), kl_div.item(), percep.item()\n",
        "\n",
        "    def train(self, num_epochs=50, verbose=True):\n",
        "        print(\"Start training!\")\n",
        "        val_time = time.time()\n",
        "        self.model.train()\n",
        "        for epoch in range(1, num_epochs + 1):\n",
        "            #self.beta = float(1 / (1 + np.exp(-0.1 * (epoch - 30))))\n",
        "            self.beta = 4\n",
        "            total_loss = total_recon = total_kl = total_percep =0\n",
        "            epoch_time = time.time()\n",
        "            print(f\"Epoch {epoch}/{num_epochs} Start, lastest epoch time cost:{epoch_time-val_time:.5f} seconds\")\n",
        "            for x, _ in self.dataloader:\n",
        "                x = x.to(self.device)\n",
        "                x_recon, mu, logvar = self.model(x)\n",
        "                loss, recon_loss, kl_loss, percep_loss = self.loss_f(x_recon, x, mu, logvar)\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                total_recon += recon_loss\n",
        "                total_kl += kl_loss\n",
        "                total_percep += percep_loss\n",
        "            val_time = time.time()\n",
        "            if verbose:\n",
        "                print(f\"Epoch {epoch}/{num_epochs} | Loss: {total_loss:.4f} | Recon: {total_recon:.4f} | KL: {self.beta*total_kl:.4f} | Percep: {0.08*total_percep:.4f}\")\n",
        "            # FID Evaluation\n",
        "            if epoch % 5 == 0:\n",
        "             with torch.no_grad():\n",
        "                self.fid_metric.reset()\n",
        "                for x, _ in self.dataloader:\n",
        "                    x = x.to(self.device)\n",
        "                    x_recon, _, _ = self.model(x)\n",
        "                    real_uint8 = (x.clamp(0,1)* 255).to(torch.uint8)\n",
        "                    fake_uint8 = (x_recon.clamp(0,1) * 255).to(torch.uint8)\n",
        "                    if real_uint8.shape[1] == 1:\n",
        "                      real_uint8 = real_uint8.repeat(1, 3, 1, 1)\n",
        "                      fake_uint8 = fake_uint8.repeat(1, 3, 1, 1)\n",
        "                    self.fid_metric.update(real_uint8, real=True)\n",
        "                    self.fid_metric.update(fake_uint8, real=False)\n",
        "\n",
        "                fid_score = self.fid_metric.compute().item()\n",
        "                print(f\"FID score: {fid_score:.2f}\")\n",
        "            self.visualize_reconstruction(epoch)\n",
        "\n",
        "        torch.save(self.model.encoder.state_dict(), self.save_path_1)\n",
        "        torch.save(self.model.decoder.state_dict(), self.save_path_2)\n",
        "        print(f\"Encoder saved to {self.save_path_1}\")\n",
        "        print(f\"Decoder saved to {self.save_path_2}\")\n",
        "    def visualize_reconstruction(self, epoch):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "\n",
        "         for x, _ in self.dataloader:\n",
        "            x = x.to(self.device)\n",
        "            x_recon, _, _ = self.model(x)\n",
        "            break\n",
        "\n",
        "        num_images = min(4, x.shape[0])\n",
        "        original = x[:num_images].cpu()\n",
        "        reconstructed = x_recon[:num_images].cpu()\n",
        "\n",
        "\n",
        "        comparison = torch.cat([original, reconstructed])\n",
        "        grid = vutils.make_grid(comparison, nrow=num_images)\n",
        "\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"Epoch {epoch} - Top: Original / Bottom: Reconstructed\")\n",
        "        plt.imshow(grid.permute(1, 2, 0).numpy())\n",
        "        plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "klncWDo6_j9y"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.4 Train on both domain and save param\n",
        "1.1->1.3->2.1->2.2->2.3"
      ],
      "metadata": {
        "id": "iQeMcTv2BDAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set load path // origin_loader , target_loader\n",
        "clear_memory()\n",
        "save_path = '/content/drive/MyDrive/VAE_models'\n",
        "VAEoencoder = os.path.join(save_path, 'VAE_O','encoder.pth')\n",
        "VAEodecoder = os.path.join(save_path, 'VAE_O','decoder.pth')\n",
        "VAEtencoder = os.path.join(save_path, 'VAE_T','encoder.pth')\n",
        "VAEtdecoder = os.path.join(save_path, 'VAE_T','decoder.pth')\n",
        "\n",
        "latent_dim = 32\n",
        "vae_origin = VAE(latent_dim=latent_dim)\n",
        "trainer_o = VAETrainer(\n",
        "    model=vae_origin,\n",
        "    dataloader= origin_loader,\n",
        "    save_path_1= VAEoencoder,\n",
        "    save_path_2= VAEodecoder,\n",
        "    lr=2e-5,\n",
        "    device=device,\n",
        "    recon_type='mse'\n",
        ")\n",
        "\n",
        "\n",
        "trainer_o.train(num_epochs=50, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "oV7YggHGBEe5",
        "outputId": "b6cd9dfc-9bfb-4dc1-882b-eb959cb35e9c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training!\n",
            "Epoch 1/50 Start, lastest epoch time cost:0.00039 seconds\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given transposed=1, weight of size [256, 128, 4, 4], expected input[16, 32, 16, 16] to have 256 channels, but got 32 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1125715076.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrainer_o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1122429918.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs, verbose)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mx_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercep_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-420978775.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_logvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mz_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mx_recon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskips\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-420978775.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z, skips)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m#x = self.fc(z).view(z.size(0), 256, 16, 16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         )\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m         return F.conv_transpose2d(\n\u001b[0m\u001b[1;32m   1163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [256, 128, 4, 4], expected input[16, 32, 16, 16] to have 256 channels, but got 32 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clear_memory()\n",
        "vae_target = VAE(latent_dim=latent_dim)\n",
        "trainer_t = VAETrainer(\n",
        "    model=vae_target,\n",
        "    dataloader=target_loader,\n",
        "    save_path_1= VAEtencoder,\n",
        "    save_path_2= VAEtdecoder,\n",
        "    lr=2e-4,\n",
        "    device=device,\n",
        "    recon_type='mse'\n",
        ")\n",
        "\n",
        "\n",
        "trainer_t.train(num_epochs=6, verbose=True)"
      ],
      "metadata": {
        "id": "MlQ5Yo3Lr_hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.5 Target domain reconstruction by VAE"
      ],
      "metadata": {
        "id": "zWR9dJ5bYbdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torchvision.utils as vutils\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "# 1->DME / 2->Normal\n",
        "def image_recon_eval(save_root=\"/content/recon_results\", n_per_class=1000):\n",
        "    if os.path.exists(save_root):\n",
        "        shutil.rmtree(save_root)\n",
        "    os.makedirs(save_root)\n",
        "    vae_cross = VAE(latent_dim=latent_dim).to(device)\n",
        "    vae_cross.encoder.load_state_dict(torch.load(VAEtencoder, map_location=device))\n",
        "    vae_cross.decoder.load_state_dict(torch.load(VAEodecoder, map_location=device))\n",
        "    vae_cross.eval()\n",
        "\n",
        "    os.makedirs(save_root, exist_ok=True)\n",
        "    class_names = target_loader.dataset.dataset.classes  # ImageFolder\n",
        "    save_counter = {cls_name: 0 for cls_name in class_names}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in target_loader:\n",
        "            x = x.to(device)\n",
        "            x_recon, _, _ = vae_cross(x)\n",
        "\n",
        "            for i in range(x.size(0)):\n",
        "                label = y[i].item()\n",
        "                cls_name = class_names[label]\n",
        "                if save_counter[cls_name] >= n_per_class:\n",
        "                    continue\n",
        "\n",
        "                cls_path = os.path.join(save_root, cls_name)\n",
        "                os.makedirs(cls_path, exist_ok=True)\n",
        "\n",
        "                recon_img = to_pil_image(x_recon[i].cpu())\n",
        "                recon_img.save(os.path.join(cls_path, f\"{save_counter[cls_name]}_recon.png\"))\n",
        "\n",
        "                save_counter[cls_name] += 1\n",
        "\n",
        "            if all(count >= n_per_class for count in save_counter.values()):\n",
        "                print(f\"Save {n_per_class} images for each class, {n_per_class * len(class_names)} in total\")\n",
        "                return\n",
        "#Run\n",
        "image_recon_eval(save_root=\"/content/recon\", n_per_class=100)\n",
        "\n"
      ],
      "metadata": {
        "id": "VoZl0byQYdUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c5HPnWbMIqoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 Run"
      ],
      "metadata": {
        "id": "7SNp2AWq0QWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trainset : (80:80)*20 / trainlabel\n",
        "# valset : 8:8\n",
        "# origin_test : 242:242\n",
        "# target_test : 1000:1000\n",
        "def Runall():\n",
        "  model = Classifier(train_loader, val_loader)\n",
        "  model.train_val()\n",
        "  res_o = model.test(test_loader_origin, \"Origin\")\n",
        "  res_t = model.test(test_loader_target, \"Target\")\n",
        "  if res_o['accuracy'] is not None:\n",
        "      print(f\"Origin: Accuracy: {res_o['accuracy']:.4f} | \"\n",
        "      f\"F1 Score: {res_o['macro avg']['f1-score']:.4f} | \"\n",
        "      f\"Precision: {res_o['macro avg']['precision']:.4f} | \"\n",
        "      f\"Recall: {res_o['macro avg']['recall']:.4f}\")\n",
        "  if res_t['accuracy'] is not None:\n",
        "      print(f\"Target: Accuracy: {res_t['accuracy']:.4f} | \"\n",
        "      f\"F1 Score: {res_t['macro avg']['f1-score']:.4f} | \"\n",
        "      f\"Precision: {res_t['macro avg']['precision']:.4f} | \"\n",
        "      f\"Recall: {res_t['macro avg']['recall']:.4f}\")\n",
        "  prototypes = model.get_prototypes(model, full_origin_loader) # full_origin_loader in block 2.1\n",
        "  res_TTA = model.test_TTA(\n",
        "    model, recon_loader, prototypes=prototypes,\n",
        "    threshold=0.9, momentum=0.8,\n",
        "    test_name=\"DME/NORMAL-TTA\"\n",
        "  )\n",
        "\n",
        "Runall()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYZFBrov0QAj",
        "outputId": "19555685-f6e8-41af-aec4-c6dc5cfcbbaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found saved model at /content/drive/MyDrive/Classifier_model/best_model.pth, loading...\n",
            "Skipping training, using loaded model.\n",
            "> Origin Start test!\n",
            "[Batch 0] | load: 0.000s | forward: 0.002s | predict: 0.083s \n",
            "[Batch 1] | load: 0.000s | forward: 0.001s | predict: 0.084s \n",
            "Origin result: 0.8512\n",
            "> Origin Finished Spend: 3.72s in total\n",
            "> Target Start test!\n",
            "[Batch 0] | load: 0.000s | forward: 0.002s | predict: 0.039s \n",
            "[Batch 1] | load: 0.000s | forward: 0.001s | predict: 0.039s \n",
            "[Batch 2] | load: 0.000s | forward: 0.001s | predict: 0.034s \n",
            "[Batch 3] | load: 0.000s | forward: 0.002s | predict: 0.034s \n",
            "[Batch 4] | load: 0.000s | forward: 0.001s | predict: 0.036s \n",
            "[Batch 5] | load: 0.000s | forward: 0.002s | predict: 0.037s \n",
            "[Batch 6] | load: 0.000s | forward: 0.001s | predict: 0.034s \n",
            "[Batch 7] | load: 0.000s | forward: 0.001s | predict: 0.035s \n",
            "[Batch 8] | load: 0.000s | forward: 0.001s | predict: 0.034s \n",
            "[Batch 9] | load: 0.000s | forward: 0.001s | predict: 0.036s \n",
            "[Batch 10] | load: 0.000s | forward: 0.001s | predict: 0.037s \n",
            "[Batch 11] | load: 0.002s | forward: 0.001s | predict: 0.040s \n",
            "[Batch 12] | load: 0.000s | forward: 0.003s | predict: 0.034s \n",
            "[Batch 13] | load: 0.000s | forward: 0.002s | predict: 0.034s \n",
            "[Batch 14] | load: 0.000s | forward: 0.002s | predict: 0.033s \n",
            "[Batch 15] | load: 0.000s | forward: 0.002s | predict: 0.034s \n",
            "[Batch 16] | load: 0.000s | forward: 0.001s | predict: 0.037s \n",
            "[Batch 17] | load: 0.000s | forward: 0.001s | predict: 0.035s \n",
            "[Batch 18] | load: 0.000s | forward: 0.001s | predict: 0.034s \n",
            "[Batch 19] | load: 0.000s | forward: 0.001s | predict: 0.035s \n",
            "Target result: 0.6255\n",
            "> Target Finished Spend: 15.21s in total\n",
            "Origin: Accuracy: 0.8512 | F1 Score: 0.8511 | Precision: 0.8524 | Recall: 0.8512\n",
            "Target: Accuracy: 0.6255 | F1 Score: 0.5881 | Precision: 0.6972 | Recall: 0.6255\n"
          ]
        }
      ]
    }
  ]
}