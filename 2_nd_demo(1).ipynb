{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMf/xZo7NkZKdRtTzuD3Vu+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnYechanJo/Novo-Nordisk_Synthetic-Image-Usage/blob/main/2_nd_demo(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Train on both domain(Kermany and OCT5K) and test the gap"
      ],
      "metadata": {
        "id": "Zi_q9XdfXV_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.1 Training Simple CNN classifier\n"
      ],
      "metadata": {
        "id": "T-UTYs00JmgU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcs1Av2ESUor",
        "outputId": "d1cb479f-dd17-4a98-c7fd-8c6272570392"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: xformers in /usr/local/lib/python3.11/dist-packages (0.0.29.post3)\n",
            "Requirement already satisfied: pytorch-fid in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.7.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.7.14)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
            "Requirement already satisfied: torch-fidelity in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (11.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (1.16.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (4.67.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install kagglehub torch torchvision transformers diffusers accelerate datasets xformers pytorch-fid pandas\n",
        "!pip install torchmetrics --upgrade torch-fidelity\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import kagglehub\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from torch.utils.data import Sampler\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "from transformers import ViTModel, CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler, StableDiffusionPipeline\n",
        "from accelerate import Accelerator\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_fid import fid_score\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "# Set random seed\n",
        "seed = 123\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# GPU setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "# Memory cleanup function\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.2 Dataset remerge(SKIP)\n",
        "Place the code file in the extracted AMD/DME/Normal directory, modify the `prefix` accordingly, and then run it. url: `https://drive.google.com/uc?id=1ZK_qMNuy53Nfxjo2vShHMFz2LyGwYAuW`\n",
        "then upload the file on drive"
      ],
      "metadata": {
        "id": "WCqhfNojkT1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os\n",
        "# set this\n",
        "prefix = 'Normal'\n",
        "def merge_all_image_dirs_under(root_dir, dst_dir, prefix=None):\n",
        "    os.makedirs(dst_dir, exist_ok=True)\n",
        "    global_index = 0\n",
        "\n",
        "    src_dirs = [\n",
        "        os.path.join(root_dir, d)\n",
        "        for d in os.listdir(root_dir)\n",
        "        if os.path.isdir(os.path.join(root_dir, d))\n",
        "    ]\n",
        "\n",
        "    for index, src_dir in enumerate(src_dirs):\n",
        "        for fname in os.listdir(src_dir):\n",
        "            if fname.lower().endswith(('.tiff')):\n",
        "                src_path = os.path.join(src_dir, fname)\n",
        "\n",
        "                if prefix:\n",
        "                    new_fname = f\"{prefix}_{global_index}.TIFF\"\n",
        "                else:\n",
        "                    new_fname = f\"{global_index}.TIFF\"\n",
        "\n",
        "                dst_path = os.path.join(dst_dir, new_fname)\n",
        "                shutil.copy(src_path, dst_path)\n",
        "                global_index += 1\n",
        "                #print(f\"{global_index} - copied from {src_dir}\")\n",
        "\n",
        "    print(f\"Successfully merged: {global_index} images in total!\")\n",
        "\n",
        "\n",
        "\n",
        "root_dir = os.getcwd() + f\"/{prefix}\"\n",
        "dst_dir = os.getcwd() + f\"/merged_{prefix}\"\n",
        "\n",
        "merge_all_image_dirs_under(root_dir, dst_dir, prefix)\n"
      ],
      "metadata": {
        "id": "8jrAtxqNkTUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.3 Dataset load and Path set"
      ],
      "metadata": {
        "id": "9etCEjPvjT1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')# mount drive\n",
        "\n",
        "# download dataset\n",
        "path = kagglehub.dataset_download(\"paultimothymooney/kermany2018\")\n",
        "loadpath = os.path.join(path, \"OCT2017 /train\")\n",
        "train_path_dme = os.path.join(loadpath, \"DME\")\n",
        "train_path_normal = os.path.join(loadpath, \"NORMAL\")\n",
        "lp = os.path.join(path, \"OCT2017 /val\")\n",
        "val_path_dme = os.path.join(lp, \"DME\")\n",
        "val_path_normal = os.path.join(lp, \"NORMAL\")\n",
        "lpp = os.path.join(path, \"OCT2017 /test\")\n",
        "test_path_dme = os.path.join(lpp, \"DME\")\n",
        "test_path_normal = os.path.join(lpp, \"NORMAL\")\n",
        "# get OCT5K from drive /content/drive/MyDrive\n",
        "target_dme = '/content/drive/MyDrive/OCT5K/DME'\n",
        "target_normal = '/content/drive/MyDrive/OCT5K/Normal'\n",
        "# 1600:1600->train 160(80:80)*20 / 8:8->val / 242:242 origin test\n",
        "# target test 1000:1000\n",
        "# define\n",
        "'''\n",
        "bsz = 160\n",
        "half_bsz = bsz // 2\n",
        "batch_num = 20\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "TH32Be2vdOna",
        "outputId": "4738849d-3261-43f3-baad-26377756fa58"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nbsz = 160\\nhalf_bsz = bsz // 2\\nbatch_num = 20\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.4 Dataloader Definition"
      ],
      "metadata": {
        "id": "5NvFpdqQW6GB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def img_transform():\n",
        "    return transforms.Compose([\n",
        "        transforms.Lambda(lambda img: img.crop((0, 100, 768, 400)) if img.size[0] >= 768 and img.size[1] >= 400 else img),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    #3，224，224\n",
        "class OCTBinaryDataset(Dataset):\n",
        "    def __init__(self, dme_dir, normal_dir, transform=None, max_per_class=None):\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "\n",
        "        dme_images = sorted([os.path.join(dme_dir, f) for f in os.listdir(dme_dir)\n",
        "                             if f.lower().endswith(('.jpg', '.jpeg', '.png','.tiff'))])\n",
        "        normal_images = sorted([os.path.join(normal_dir, f) for f in os.listdir(normal_dir)\n",
        "                                if f.lower().endswith(('.jpg', '.jpeg', '.png','.tiff'))])\n",
        "\n",
        "        if max_per_class:\n",
        "            dme_images = dme_images[:max_per_class]\n",
        "            normal_images = normal_images[:max_per_class]\n",
        "\n",
        "        self.dme_paths = dme_images\n",
        "        self.normal_paths = normal_images\n",
        "        assert len(self.dme_paths) > 0, f\"No images found in {dme_dir}\"\n",
        "        assert len(self.normal_paths) > 0, f\"No images found in {normal_dir}\"\n",
        "        self.samples += [(path, 1) for path in dme_images]      # DME: label 1\n",
        "        self.samples += [(path, 0) for path in normal_images]   # NORMAL: label 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "class BalancedBinaryBatchSampler(Sampler):\n",
        "    def __init__(self, dme_len, normal_len, batch_size, num_batches=None, seed = 123):\n",
        "        assert batch_size % 2 == 0, \"Batch size must be divisible by 2\"\n",
        "        self.dme_len = dme_len\n",
        "        self.normal_len = normal_len\n",
        "        self.half_batch = batch_size // 2\n",
        "        self.num_batches = num_batches or min(dme_len, normal_len) // self.half_batch\n",
        "        self.seed = seed\n",
        "        assert self.dme_len >= self.half_batch * self.num_batches, \"Not enough DME samples\"\n",
        "        assert self.normal_len >= self.half_batch * self.num_batches, \"Not enough NORMAL samples\"\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        rng = random.Random(self.seed)\n",
        "        dme_indices = list(range(self.dme_len))\n",
        "        normal_indices = list(range(self.normal_len))\n",
        "        rng.shuffle(dme_indices)\n",
        "        rng.shuffle(normal_indices)\n",
        "\n",
        "        for i in range(self.num_batches):\n",
        "            dme_batch = dme_indices[i*self.half_batch:(i+1)*self.half_batch]\n",
        "            normal_batch = normal_indices[i*self.half_batch:(i+1)*self.half_batch]\n",
        "            yield dme_batch + [j + self.dme_len for j in normal_batch]  # shift normal index\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_batches\n",
        "\n",
        "def build_train_dataloader(dme_dir, normal_dir, batch_size=160, num_batches=20, transform=None, max_per_class=None):\n",
        "    transform = transform or img_transform()\n",
        "    dataset = OCTBinaryDataset(dme_dir, normal_dir, transform=transform, max_per_class=max_per_class)\n",
        "    dme_len = len(dataset.dme_paths)\n",
        "    normal_len = len(dataset.normal_paths)\n",
        "    sampler = BalancedBinaryBatchSampler(dme_len, normal_len, batch_size=batch_size, num_batches = num_batches)\n",
        "    def balanced_collate_fn(batch):\n",
        "        random.shuffle(batch)\n",
        "        imgs, labels = zip(*batch)\n",
        "        return torch.stack(imgs), torch.tensor(labels)\n",
        "    loader = DataLoader(dataset, batch_sampler=sampler,\n",
        "                        collate_fn=balanced_collate_fn,\n",
        "                        num_workers=2, pin_memory=True)\n",
        "    return loader\n",
        "def build_valandtest_dataloader(dme_dir, normal_dir, batch_size, transform=None, max_per_class=None):\n",
        "    transform = transform or img_transform()\n",
        "    dataset = OCTBinaryDataset(dme_dir, normal_dir, transform=transform, max_per_class=max_per_class)\n",
        "    dme_len = len(dataset.dme_paths)\n",
        "    normal_len = len(dataset.normal_paths)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size,\n",
        "                        shuffle=False,\n",
        "                        num_workers=2, pin_memory=True)\n",
        "    return loader\n",
        "\n",
        "train_loader = build_train_dataloader(\n",
        "    dme_dir= train_path_dme ,\n",
        "    normal_dir= train_path_normal,\n",
        "    batch_size=160,\n",
        "    num_batches = 20,\n",
        "    transform=img_transform(),\n",
        "    max_per_class=1600\n",
        ")\n",
        "\n",
        "val_loader = build_valandtest_dataloader(\n",
        "    dme_dir= val_path_dme ,\n",
        "    normal_dir= val_path_normal,\n",
        "    batch_size=8,\n",
        "    transform=img_transform(),\n",
        "    max_per_class = 8\n",
        ")\n",
        "\n",
        "test_loader_origin = build_valandtest_dataloader(\n",
        "    dme_dir= test_path_dme ,\n",
        "    normal_dir= test_path_normal,\n",
        "    batch_size=242,\n",
        "    transform=img_transform(),\n",
        "    max_per_class = 242\n",
        ")\n",
        "\n",
        "# may cost extra time to load when test for the first time\n",
        "test_loader_target = build_valandtest_dataloader(\n",
        "    dme_dir= target_dme ,\n",
        "    normal_dir= target_normal,\n",
        "    batch_size=100,\n",
        "    transform=img_transform(),\n",
        "    max_per_class = 1000\n",
        ")\n"
      ],
      "metadata": {
        "id": "4JUssCqwe5mZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.5 Clssifier Definition"
      ],
      "metadata": {
        "id": "lD-Hvx8ejrqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),  # (3,224,224) -> (16,224,224)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                            # -> (16,112,112)\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1), # -> (32,112,112)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                             # -> (32,56,56)\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # -> (64,56,56)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)                              # -> (64,28,28)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),             # -> (64×28×28)\n",
        "            nn.Linear(64*28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2)         # 2-class classification\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, train_loader, val_loader):\n",
        "        super().__init__()\n",
        "        self.model = SimpleCNN().cuda()\n",
        "        self.best_acc = 0\n",
        "        self.model_path = \"/content/drive/MyDrive/Classifier_model/best_model.pth\"\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "\n",
        "\n",
        "    def load_model(self):\n",
        "        if os.path.exists(self.model_path):\n",
        "            print(f\"Found saved model at {self.model_path}, loading...\")\n",
        "            checkpoint = torch.load(self.model_path,map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.model.eval()\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def train_val(self):\n",
        "      if self.load_model():\n",
        "        print(\"Skipping training, using loaded model.\")\n",
        "      else:\n",
        "        print(\"No saved model found, starting training.\")\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
        "        loss = nn.CrossEntropyLoss()\n",
        "        epochs = 10\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "            # train\n",
        "            self.model.train()\n",
        "            for i, data in enumerate(self.train_loader):\n",
        "                total = len(self.train_loader)\n",
        "                batch_x, batch_y = (item.cuda() for item in data)\n",
        "                self.optimizer.zero_grad()\n",
        "                logit_original = self.model(batch_x)\n",
        "                l = loss(logit_original, batch_y)\n",
        "                l.backward()\n",
        "                self.optimizer.step()\n",
        "                corrects = (torch.max(logit_original, 1)[1].view(batch_y.size()).data == batch_y.data).sum()\n",
        "                accuracy = 100 * corrects / len(batch_y)\n",
        "                print(f'Batch[{i + 1}/{total}] - loss: {l.item():.6f}  accuracy: {accuracy:.4f}%({corrects}/{batch_y.size(0)})')\n",
        "            # val\n",
        "            self.model.eval()\n",
        "            all_preds, all_labels = [], []\n",
        "            with torch.no_grad():\n",
        "                for x, y in self.val_loader:\n",
        "                    x = x.cuda()\n",
        "                    logits = self.model(x)\n",
        "                    preds = torch.argmax(logits, dim=1)\n",
        "                    all_preds.extend(preds.cpu().tolist())\n",
        "                    all_labels.extend(y.tolist())\n",
        "\n",
        "                acc = accuracy_score(all_labels, all_preds)\n",
        "                print(f\"Validation Accuracy: {acc:.4f}\")\n",
        "                if acc > self.best_acc:\n",
        "                   self.best_acc = acc\n",
        "                   os.makedirs(os.path.dirname(self.model_path), exist_ok=True)\n",
        "                   torch.save({\n",
        "                        'model_state_dict': self.model.state_dict()\n",
        "                    }, self.model_path)\n",
        "                   print(f\"Model saved at {self.model_path}\")\n",
        "                print(\"Best val set acc:\", self.best_acc)\n",
        "\n",
        "\n",
        "    # test\n",
        "    def test(self, loader, test_name=\"\"):\n",
        "       self.model.eval()\n",
        "       all_preds, all_labels = [], []\n",
        "       start_time = time.time()\n",
        "       print(f\"> {test_name} Start test!\")\n",
        "\n",
        "       with torch.no_grad():\n",
        "         for i,data in enumerate(loader):\n",
        "            t0 = time.time()\n",
        "            x,y = (item.cuda(non_blocking=True) for item in data)\n",
        "            t1 = time.time()\n",
        "            logits = self.model(x)\n",
        "            t2 = time.time()\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().tolist())\n",
        "            all_labels.extend(y.tolist())\n",
        "            t3 = time.time()\n",
        "            print(f\"[Batch {i}] | load: {t1 - t0:.3f}s | forward: {t2 - t1:.3f}s | predict: {t3 - t2:.3f}s \")\n",
        "       report = classification_report(\n",
        "           all_labels, all_preds,\n",
        "           labels=[0, 1],\n",
        "           target_names=['NR', 'FR'],\n",
        "           digits=3,\n",
        "           output_dict=True\n",
        "       )\n",
        "       print(f\"{test_name} result: {report['accuracy']:.4f}\")\n",
        "       total_time = time.time() - start_time\n",
        "       print(f\"> {test_name} Finished Spend: {total_time:.2f}s in total\")\n",
        "       return report\n"
      ],
      "metadata": {
        "id": "5i_6uTgFdcDA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.6 Run"
      ],
      "metadata": {
        "id": "7SNp2AWq0QWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trainset : (80:80)*20 / trainlabel\n",
        "# valset : 8:8\n",
        "# origin_test : 242:242\n",
        "# target_test : 1000:1000\n",
        "def Runall():\n",
        "  model = Classifier(train_loader, val_loader)\n",
        "  model.train_val()\n",
        "  res_o = model.test(test_loader_origin, \"Origin\")\n",
        "  res_t = model.test(test_loader_target, \"Target\")\n",
        "  if res_o['accuracy'] is not None:\n",
        "      print(f\"Origin: Accuracy: {res_o['accuracy']:.4f} | \"\n",
        "      f\"F1 Score: {res_o['macro avg']['f1-score']:.4f} | \"\n",
        "      f\"Precision: {res_o['macro avg']['precision']:.4f} | \"\n",
        "      f\"Recall: {res_o['macro avg']['recall']:.4f}\")\n",
        "  if res_t['accuracy'] is not None:\n",
        "      print(f\"Target: Accuracy: {res_t['accuracy']:.4f} | \"\n",
        "      f\"F1 Score: {res_t['macro avg']['f1-score']:.4f} | \"\n",
        "      f\"Precision: {res_t['macro avg']['precision']:.4f} | \"\n",
        "      f\"Recall: {res_t['macro avg']['recall']:.4f}\")\n",
        "Runall()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYZFBrov0QAj",
        "outputId": "19555685-f6e8-41af-aec4-c6dc5cfcbbaa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found saved model at /content/drive/MyDrive/Classifier_model/best_model.pth, loading...\n",
            "Skipping training, using loaded model.\n",
            "> Origin Start test!\n",
            "[Batch 0] | load: 0.000s | forward: 0.002s | predict: 0.083s \n",
            "[Batch 1] | load: 0.000s | forward: 0.001s | predict: 0.084s \n",
            "Origin result: 0.8512\n",
            "> Origin Finished Spend: 3.72s in total\n",
            "> Target Start test!\n",
            "[Batch 0] | load: 0.000s | forward: 0.002s | predict: 0.039s \n",
            "[Batch 1] | load: 0.000s | forward: 0.001s | predict: 0.039s \n",
            "[Batch 2] | load: 0.000s | forward: 0.001s | predict: 0.034s \n",
            "[Batch 3] | load: 0.000s | forward: 0.002s | predict: 0.034s \n",
            "[Batch 4] | load: 0.000s | forward: 0.001s | predict: 0.036s \n",
            "[Batch 5] | load: 0.000s | forward: 0.002s | predict: 0.037s \n",
            "[Batch 6] | load: 0.000s | forward: 0.001s | predict: 0.034s \n",
            "[Batch 7] | load: 0.000s | forward: 0.001s | predict: 0.035s \n",
            "[Batch 8] | load: 0.000s | forward: 0.001s | predict: 0.034s \n",
            "[Batch 9] | load: 0.000s | forward: 0.001s | predict: 0.036s \n",
            "[Batch 10] | load: 0.000s | forward: 0.001s | predict: 0.037s \n",
            "[Batch 11] | load: 0.002s | forward: 0.001s | predict: 0.040s \n",
            "[Batch 12] | load: 0.000s | forward: 0.003s | predict: 0.034s \n",
            "[Batch 13] | load: 0.000s | forward: 0.002s | predict: 0.034s \n",
            "[Batch 14] | load: 0.000s | forward: 0.002s | predict: 0.033s \n",
            "[Batch 15] | load: 0.000s | forward: 0.002s | predict: 0.034s \n",
            "[Batch 16] | load: 0.000s | forward: 0.001s | predict: 0.037s \n",
            "[Batch 17] | load: 0.000s | forward: 0.001s | predict: 0.035s \n",
            "[Batch 18] | load: 0.000s | forward: 0.001s | predict: 0.034s \n",
            "[Batch 19] | load: 0.000s | forward: 0.001s | predict: 0.035s \n",
            "Target result: 0.6255\n",
            "> Target Finished Spend: 15.21s in total\n",
            "Origin: Accuracy: 0.8512 | F1 Score: 0.8511 | Precision: 0.8524 | Recall: 0.8512\n",
            "Target: Accuracy: 0.6255 | F1 Score: 0.5881 | Precision: 0.6972 | Recall: 0.6255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. CNN-Based VAE"
      ],
      "metadata": {
        "id": "jabI_wxzis8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.1 Dataloader"
      ],
      "metadata": {
        "id": "t_alDV0j34_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Subset\n",
        "import shutil\n",
        "import torch.nn.functional as F\n",
        "from collections import defaultdict\n",
        "def img_transform():\n",
        "    return transforms.Compose([\n",
        "        transforms.Lambda(lambda img: img.crop((0, 100, 768, 400)) if img.size[0] >= 768 and img.size[1] >= 400 else img),\n",
        "        transforms.Resize((256,256)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "def get_limited_indices(dataset, target_labels, max_per_class):\n",
        "    label_counts = defaultdict(int)\n",
        "    selected_indices = []\n",
        "\n",
        "    for idx, label in enumerate(dataset.targets):\n",
        "        if label in target_labels and label_counts[label] < max_per_class:\n",
        "            selected_indices.append(idx)\n",
        "            label_counts[label] += 1\n",
        "\n",
        "        # early stop\n",
        "        if all(label_counts[l] >= max_per_class for l in target_labels):\n",
        "            break\n",
        "\n",
        "    return selected_indices\n",
        "\n",
        "if os.path.exists('/content/OCT5K'):\n",
        "    shutil.rmtree('/content/OCT5K')\n",
        "shutil.copytree('/content/drive/MyDrive/OCT5K', '/content/OCT5K')\n",
        "lp_target = '/content/OCT5K'\n",
        "transform = img_transform()\n",
        "origin_dataset = ImageFolder(root=loadpath, transform=transform)\n",
        "target_dataset = ImageFolder(root=lp_target, transform=transform)\n",
        "print(origin_dataset.class_to_idx)\n",
        "print(target_dataset.class_to_idx)\n",
        "# deal with iamgefolder issue: it treat each folder as an auto label for one class\n",
        "# but we do binary, since dme/normal is the maximum intersection between two datasets\n",
        "max_images_per_class = 1000\n",
        "origin_indices = get_limited_indices(origin_dataset, target_labels=[1, 3], max_per_class=max_images_per_class)\n",
        "target_indices = get_limited_indices(target_dataset, target_labels=[1, 2], max_per_class=max_images_per_class)\n",
        "origin_subset = Subset(origin_dataset, origin_indices)\n",
        "target_subset = Subset(target_dataset, target_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sx3DvLKoitcG",
        "outputId": "124f7934-b29c-4a00-85c0-4a0f774928f9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'CNV': 0, 'DME': 1, 'DRUSEN': 2, 'NORMAL': 3}\n",
            "{'AMD': 0, 'DME': 1, 'Normal': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "origin_loader = DataLoader(origin_subset, batch_size=16, shuffle=True, num_workers=2)\n",
        "target_loader = DataLoader(target_subset, batch_size=16, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "C6E05wcAqFzX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.2 VAE Definition"
      ],
      "metadata": {
        "id": "8vjfibh438yT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "        self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=1) \\\n",
        "            if in_channels != out_channels else nn.Identity()\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activation(self.block(x) + self.skip(x))\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dim=32):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            ResidualBlock(3, 32),     # (B,3,256,256) -> (B,32,256,256)\n",
        "            nn.MaxPool2d(2),          # -> (B,32,128,128)\n",
        "\n",
        "            ResidualBlock(32, 64),    # -> (B,64,128,128)\n",
        "            nn.MaxPool2d(2),          # -> (B,64,64,64)\n",
        "\n",
        "            ResidualBlock(64, 128),   # -> (B,128,64,64)\n",
        "            nn.MaxPool2d(2),          # -> (B,128,32,32)\n",
        "\n",
        "            ResidualBlock(128, 256),  # -> (B,256,32,32)\n",
        "            nn.MaxPool2d(2),          # -> (B,256,16,16)\n",
        "\n",
        "            nn.Conv2d(256, latent_dim, kernel_size=1),  # -> (B,latent_dim,16,16)\n",
        "            nn.AdaptiveAvgPool2d((1, 1))                # -> (B,latent_dim,1,1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return x.view(x.size(0), -1)  # -> (B, latent_dim)\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim=32):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(latent_dim, 256 * 8 * 8)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # -> (128,16,16)\n",
        "            ResidualBlock(128, 128),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # -> (64,32,32)\n",
        "            ResidualBlock(64, 64),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),    # -> (32,64,64)\n",
        "            ResidualBlock(32, 32),\n",
        "\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),    # -> (16,128,128)\n",
        "            ResidualBlock(16, 16),\n",
        "\n",
        "            nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=1),     # -> (3,256,256)\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)                        # -> (B,256*8*8)\n",
        "        x = x.view(x.size(0), 256, 8, 8)      # -> (B,256,8,8)\n",
        "        return self.decoder(x)\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim=32):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(latent_dim=latent_dim)\n",
        "        self.fc_mu = nn.Linear(latent_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(latent_dim, latent_dim)\n",
        "        self.decoder_input = nn.Sequential(\n",
        "            nn.Linear(latent_dim, latent_dim),\n",
        "            nn.GELU()  # 或 GELU\n",
        "        )\n",
        "        self.decoder = Decoder(latent_dim=latent_dim)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        mu = self.fc_mu(z)\n",
        "        logvar = self.fc_logvar(z)\n",
        "        z_sample = self.reparameterize(mu, logvar)\n",
        "        x_recon = self.decoder(self.decoder_input(z_sample))\n",
        "        return x_recon, mu, logvar\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "Q6rMqMwB4B26"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.3 VAE Trainer Definition"
      ],
      "metadata": {
        "id": "_W5zSEcv_kSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "\n",
        "class VGGPerceptualLoss(nn.Module):\n",
        "    def __init__(self, layers=['relu3_3'], resize=True):\n",
        "        super().__init__()\n",
        "        vgg = models.vgg16(pretrained=True).features.eval()\n",
        "        self.resize = resize\n",
        "\n",
        "        self.selected_layers = {\n",
        "            'relu1_2': 3,\n",
        "            'relu2_2': 8,\n",
        "            'relu3_3': 15,\n",
        "            'relu4_3': 22,\n",
        "        }\n",
        "        self.layer_index = self.selected_layers[layers[0]]\n",
        "\n",
        "        self.vgg_part = nn.Sequential(*list(vgg.children())[:self.layer_index + 1])\n",
        "        for p in self.vgg_part.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).cuda()\n",
        "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).cuda()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "\n",
        "        if x.shape[1] == 1:\n",
        "            x = x.repeat(1, 3, 1, 1)\n",
        "            y = y.repeat(1, 3, 1, 1)\n",
        "\n",
        "        x = (x - self.mean) / self.std\n",
        "        y = (y - self.mean) / self.std\n",
        "\n",
        "        feat_x = self.vgg_part(x)\n",
        "        feat_y = self.vgg_part(y)\n",
        "        loss = nn.functional.l1_loss(feat_x, feat_y)\n",
        "        return loss\n",
        "\n",
        "\n",
        "class VAETrainer:\n",
        "    def __init__(self, model, dataloader, save_path_1, save_path_2,\n",
        "                 lr=2e-4, device='cuda', beta=1.0,\n",
        "                 recon_type='mse'):\n",
        "        self.model = model.to(device)\n",
        "        self.dataloader = dataloader\n",
        "        self.save_path_1 = save_path_1\n",
        "        self.save_path_2 = save_path_2\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        self.device = device\n",
        "        self.beta = beta\n",
        "        self.recon_type = recon_type\n",
        "        self.percep_loss = VGGPerceptualLoss().to(self.device)\n",
        "        self.fid_metric = FrechetInceptionDistance(feature=2048).to(self.device)\n",
        "\n",
        "    def loss_f(self, recon_x, x, mu, logvar):\n",
        "        if self.recon_type == 'mse':\n",
        "            recon_loss = F.mse_loss(recon_x, x, reduction='mean')\n",
        "        elif self.recon_type == 'l1':\n",
        "            recon_loss = F.l1_loss(recon_x, x, reduction='mean')\n",
        "        else:\n",
        "            raise ValueError(\"recon_type must be 'mse' or 'l1'\")\n",
        "\n",
        "        kl_div = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        L1_loss = F.l1_loss(recon_x, x, reduction='mean')\n",
        "        percep = self.percep_loss(recon_x, x)\n",
        "        total_loss = 0.9 * recon_loss + 0.1 * L1_loss +  0.2 * percep + self.beta * kl_div\n",
        "        return total_loss, recon_loss.item(), kl_div.item(), percep.item()\n",
        "\n",
        "    def train(self, num_epochs=50, verbose=True):\n",
        "        print(\"Start training!\")\n",
        "        val_time = time.time()\n",
        "        self.model.train()\n",
        "        for epoch in range(1, num_epochs + 1):\n",
        "            self.beta = float(1 / (1 + np.exp(-0.1 * (epoch - 50))))\n",
        "            total_loss = total_recon = total_kl = total_percep =0\n",
        "            epoch_time = time.time()\n",
        "            print(f\"Epoch {epoch}/{num_epochs} Start, lastest epoch time cost:{epoch_time-val_time:.5f} seconds\")\n",
        "            for x, _ in self.dataloader:\n",
        "                x = x.to(self.device)\n",
        "                x_recon, mu, logvar = self.model(x)\n",
        "                loss, recon_loss, kl_loss, percep_loss = self.loss_f(x_recon, x, mu, logvar)\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                total_recon += recon_loss\n",
        "                total_kl += kl_loss\n",
        "                total_percep += percep_loss\n",
        "            val_time = time.time()\n",
        "            if verbose:\n",
        "                print(f\"Epoch {epoch}/{num_epochs} | Loss: {total_loss:.4f} | Recon: {total_recon:.4f} | KL: {total_kl:.4f} | Percep: {total_percep:.4f}\")\n",
        "            # FID Evaluation\n",
        "            with torch.no_grad():\n",
        "                self.fid_metric.reset()\n",
        "                for x, _ in self.dataloader:\n",
        "                    x = x.to(self.device)\n",
        "                    x_recon, _, _ = self.model(x)\n",
        "                    real_uint8 = (x.clamp(0,1)* 255).to(torch.uint8)\n",
        "                    fake_uint8 = (x_recon.clamp(0,1) * 255).to(torch.uint8)\n",
        "                    self.fid_metric.update(real_uint8, real=True)\n",
        "                    self.fid_metric.update(fake_uint8, real=False)\n",
        "                    break\n",
        "                fid_score = self.fid_metric.compute().item()\n",
        "                print(f\"FID score: {fid_score:.2f}\")\n",
        "\n",
        "        torch.save(self.model.encoder.state_dict(), self.save_path_1)\n",
        "        torch.save(self.model.decoder.state_dict(), self.save_path_2)\n",
        "        print(f\"Encoder saved to {self.save_path_1}\")\n",
        "        print(f\"Decoder saved to {self.save_path_2}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "klncWDo6_j9y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.4 Train on both domain and save param\n",
        "1.1->1.3->2.1->2.2->2.3"
      ],
      "metadata": {
        "id": "iQeMcTv2BDAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set load path // origin_loader , target_loader\n",
        "clear_memory()\n",
        "save_path = '/content/drive/MyDrive/VAE_models'\n",
        "VAEoencoder = os.path.join(save_path, 'VAE_O','encoder.pth')\n",
        "VAEodecoder = os.path.join(save_path, 'VAE_O','decoder.pth')\n",
        "VAEtencoder = os.path.join(save_path, 'VAE_T','encoder.pth')\n",
        "VAEtdecoder = os.path.join(save_path, 'VAE_T','decoder.pth')\n",
        "\n",
        "latent_dim = 32\n",
        "vae_origin = VAE(latent_dim=latent_dim)\n",
        "trainer_o = VAETrainer(\n",
        "    model=vae_origin,\n",
        "    dataloader=origin_loader,\n",
        "    save_path_1= VAEoencoder,\n",
        "    save_path_2= VAEodecoder,\n",
        "    lr=2e-4,\n",
        "    device=device,\n",
        "    recon_type='mse'\n",
        ")\n",
        "\n",
        "\n",
        "trainer_o.train(num_epochs=50, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oV7YggHGBEe5",
        "outputId": "576c909f-495d-42cf-aba7-49eccb96d349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training!\n",
            "Epoch 1/50 Start, lastest epoch time cost:0.00053 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clear_memory()\n",
        "vae_target = VAE(latent_dim=latent_dim)\n",
        "trainer_t = VAETrainer(\n",
        "    model=vae_target,\n",
        "    dataloader=target_loader,\n",
        "    save_path_1= VAEtencoder,\n",
        "    save_path_2= VAEtdecoder,\n",
        "    lr=2e-4,\n",
        "    device=device,\n",
        "    recon_type='mse'\n",
        ")\n",
        "\n",
        "\n",
        "trainer_t.train(num_epochs=6, verbose=True)"
      ],
      "metadata": {
        "id": "MlQ5Yo3Lr_hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.5 Target domain reconstruction by VAE"
      ],
      "metadata": {
        "id": "zWR9dJ5bYbdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torchvision.utils as vutils\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "# 1->DME / 2->Normal\n",
        "def image_recon_eval(save_root=\"/content/recon_results\", n_per_class=1000):\n",
        "    if os.path.exists(save_root):\n",
        "        shutil.rmtree(save_root)\n",
        "    os.makedirs(save_root)\n",
        "    vae_cross = VAE(latent_dim=latent_dim).to(device)\n",
        "    vae_cross.encoder.load_state_dict(torch.load(VAEtencoder, map_location=device))\n",
        "    vae_cross.decoder.load_state_dict(torch.load(VAEodecoder, map_location=device))\n",
        "    vae_cross.eval()\n",
        "\n",
        "    os.makedirs(save_root, exist_ok=True)\n",
        "    class_names = target_loader.dataset.dataset.classes  # ImageFolder\n",
        "    save_counter = {cls_name: 0 for cls_name in class_names}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in target_loader:\n",
        "            x = x.to(device)\n",
        "            x_recon, _, _ = vae_cross(x)\n",
        "\n",
        "            for i in range(x.size(0)):\n",
        "                label = y[i].item()\n",
        "                cls_name = class_names[label]\n",
        "                if save_counter[cls_name] >= n_per_class:\n",
        "                    continue\n",
        "\n",
        "                cls_path = os.path.join(save_root, cls_name)\n",
        "                os.makedirs(cls_path, exist_ok=True)\n",
        "\n",
        "                recon_img = to_pil_image(x_recon[i].cpu())\n",
        "                recon_img.save(os.path.join(cls_path, f\"{save_counter[cls_name]}_recon.png\"))\n",
        "\n",
        "                save_counter[cls_name] += 1\n",
        "\n",
        "            if all(count >= n_per_class for count in save_counter.values()):\n",
        "                print(f\"Save {n_per_class} images for each class, {n_per_class * len(class_names)} in total\")\n",
        "                return\n",
        "#Run\n",
        "image_recon_eval(save_root=\"/content/recon\", n_per_class=100)\n",
        "\n"
      ],
      "metadata": {
        "id": "VoZl0byQYdUv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}